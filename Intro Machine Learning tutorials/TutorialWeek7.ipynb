{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# 2022-06-08 Wednesday\n",
    "\n",
    "NAME: Ravi Moelchand - 7463332 \\\n",
    "Topics covered since the last tutorial:\n",
    "\n",
    "* Cross-Validation, Non-Linear Transformations, Generalization (Lecture 10, 2022-06-03)\n",
    "\n",
    "* Logistic Regression, Gradient Descent (Lecture 11, 2022-06-08)\n",
    "\n",
    "And we will also cover this mathematics topic:\n",
    "\n",
    "* Expectations; Continuous Random Variables (week 6)\n",
    "\n",
    "If you write your answers directly into the notebook, it is preferred that you generate a .pdf file for submission\n",
    "\n",
    "**This tutorial contains 10 problems.** Please submit one solution per person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting, Learning Principles\n",
    "\n",
    "**1.** Suppose we want to train a machine learning model using 800 datapoints (we have a separate test set that will be used for testing at the end). During the training phase, we would like to tune certain hyperparameters of the model, such as (for example) the learning rate. To do that, we decide to use 5-fold cross-validation. How many training datapoints will be used to train each model?\n",
    "\n",
    "**answer**\n",
    "\n",
    "*Since we have a separate test set we don't use one of these 5 'folds' of data for testing, but we will use one as a validation set. Therefore we will use 800-(800/5) = 640 datapoints to train the model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "**2.** Exercise 3.5 from the book:\n",
    "\n",
    "In logistic regression, the hypothesis function needs to return values between 0 and 1 (to be interpreted as probabilities). The default choice is the *logistic* function $\\theta=e^s/(1+e^s)$. This function provides a *soft threshold*, as opposed to the *hard threshold* from classification.\n",
    "\n",
    "Another popular choice is the *hyperbolic tangent*:\n",
    "\n",
    "\n",
    "(a) Show the mathematical relation between tanh and $\\theta$.\n",
    "\n",
    "**answer**\n",
    "\n",
    "${\\rm tanh}(s) = \\frac{e^s-e^{-s}}{e^s+e^{-s}}$\\\n",
    "$\\frac{e^s-e^{-s}}{e^s+e^{-s}} = \\frac{e^s - \\frac{1}{e^s}}{e^s\\frac{1}{e^s}}$\\\n",
    "$\\frac{e^s - \\frac{1}{e^s}}{e^s\\frac{1}{e^s}} = \\frac{e^{2s}-1}{e^{2s}+1}$\\\n",
    "$\\frac{e^{2s}-1}{e^{2s}+1} = \\frac{e^{2s}}{1+e^{2s}} - \\frac{1}{1+e^{2s}}$\\\n",
    "$\\frac{e^{2s}}{1+e^{2s}} - \\frac{1}{1+e^{2s}} = \\theta (2s) - \\frac{1}{1+e^{2s}}$\\\n",
    "$\\theta (2s) - \\frac{1}{1+e^{2s}} = \\theta (2s) - (1 - \\theta (2s))$\\\n",
    "$2\\theta (2s) - 1$\n",
    "    \n",
    "\n",
    "(b) Show that tanh$(s)$ converges to a hard threshold (+1 or -1) for large $|s|$, and converges to no threshold (no binary predictions) for small $|s|$.\n",
    "\n",
    "**answer**\n",
    "\n",
    "*The formula ${\\rm tanh}(s) = \\frac{e^s-e^{-s}}{e^s+e^{-s}}$ can be rewritten as  ${\\rm tanh}(s) = \\frac{1-e^{-2s}}{1+e^{-2s}}$. Now if we let s approach $\\infty$, the value of $e^{-2s}$ will approach 0 because we divide 1 by a very large number. Our ${\\rm tanh}(s)$ will then approach 1, since $\\frac{1-0}{1+0} = \\frac{1}{1} = 1$. When we let s approach $-\\infty$, through a similar process, ${\\rm tanh}(s)$ will approach -1. So this results in a hard treshold.*\n",
    "\n",
    "*When s is small the opposite happens and ${\\rm tanh}(s)$ will approach 0, which means that there is no treshold.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Exercise 3.6 from the book (cross-entropy error measure)\n",
    "\n",
    "(a) If we are learning from +1/-1 data to predict a noisy target $P(y|\\mathbf{x})$ with candidate hypothesis $h$, show that the maximum likelihood method reduces to the task of finding $h$ that minimizes\n",
    "\n",
    "$E_{\\rm in}(\\mathbf{w})=\\sum_{n=1}^N [[y_n=+1]] {\\rm ln}\\frac{1}{h(\\mathbf{x}_n)} + [[y_n=-1]] {\\rm ln} \\frac{1}{1-h(\\mathbf{x}_n)}$\n",
    "\n",
    "**answer**\n",
    "\n",
    "\n",
    "$E_{in}(w) = \\sum_{n=1}^N {\\rm ln} (P(y_n|x_n)) = \\sum_{n=1}^N [[y_n=+1]] {\\rm ln}\\frac{1}{h(\\mathbf{x}_n)} + [[y_n=-1]] {\\rm ln} \\frac{1}{1-h(\\mathbf{x}_n)}$\n",
    "\n",
    "(b) For the case $h(\\mathbf{x})=\\theta(\\mathbf{w}^{\\rm T}\\mathbf{x})$, argue that minimizing the in-sample error in part (a) is equivalent to minimizing Equation 3.9 of the book:\n",
    "\n",
    "$E_{\\rm in}(\\mathbf{w}) = \\frac{1}{N}\\sum^N_{n=1}{\\rm ln}\\left(1+e^{-y_n\\mathbf{w}^{\\rm T}\\mathbf{x}_n}\\right)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Exercise 3.7 from the book:\n",
    "\n",
    "For logistic regression, show that\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla E_{\\rm in}(\\mathbf{w}) &= -\\frac{1}{N}\\sum^{N}_{n=1}\\frac{y_n\\mathbf{x}_n}{1+e^{y_n\\mathbf{w}^{\\rm T}\\mathbf{x}_n}} \\\\\n",
    "&=\\frac{1}{N}\\sum^N_{n=1}-y_n\\mathbf{x}_n\\theta(-y_n\\mathbf{w}^{\\rm T}\\mathbf{x}_n) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "**answer**\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla E_{\\rm in}(\\mathbf{w}) &= -\\frac{1}{N}\\sum^{N}_{n=1}\\frac{y_n\\mathbf{x}_{n}e^{-y_n\\mathbf{w}^{\\rm T}\\mathbf{x}_n}}{1+e^{-y_n\\mathbf{w}^{\\rm T}\\mathbf{x}_n}}\\\\\n",
    "&= -\\frac{1}{N}\\sum^{N}_{n=1}\\frac{y_n\\mathbf{x}_n}{1+e^{y_n\\mathbf{w}^{\\rm T}\\mathbf{x}_n}} \\\\\n",
    "&=\\frac{1}{N}\\sum^N_{n=1}-y_n\\mathbf{x}_n\\theta(-y_n\\mathbf{w}^{\\rm T}\\mathbf{x}_n) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Following up on problem 4, argue that a \"misclassified\" example contributes more to the gradient than a correctly classified one\n",
    "\n",
    "**answer**\n",
    "\n",
    "When a sample is misclassified, $\\theta(-y_n\\mathbf{w}^{\\rm T}\\mathbf{x}_n) > 0.5$, while when a sample is correctly classified, $\\theta(-y_n\\mathbf{w}^{\\rm T}\\mathbf{x}_n) < 0.5$, so the contribution to the gradient of a misclassified example is larger than the contribution to the gradient of a correctly classified one. \n",
    "\n",
    "source: https://github.com/niuers/Learning-From-Data-A-Short-Course/blob/master/Solutions%20to%20Chapter%203%20The%20Linear%20Model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.** Exercise 3.8 in the book:\n",
    "\n",
    "In section 3.3.2 of the book we learn about $\\hat{\\mathbf{v}}$, a unit vector in whose direction we take a step during gradient descent. Equation 3.10 gives us the formula for this vector, and the book claims that this direction leads to the largest decrease in $E_{\\rm in}$ for a given step size $\\eta$. This claim only holds for small $\\eta$. Why?\n",
    "\n",
    "**answer**\n",
    "\n",
    "$\\nu$ is the direction which gives largest decrease in E_{in}\n",
    " only holds for small $\\eta$, that's because when  is large, we can't ignore the squared term and smaller terms in the Taylor expansion. The lower bound can't be achieved. \n",
    " \n",
    "source: https://github.com/niuers/Learning-From-Data-A-Short-Course/blob/master/Solutions%20to%20Chapter%203%20The%20Linear%20Model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectations; Continuous Random Variables\n",
    "\n",
    "Refresh the basic notions of the mean of a probability distibution and the mean of a random variable. We recommend *Probability* (1993) by Jim Pitman, section 3.2, pp.162-163.\n",
    "\n",
    "**7.** Are there probability distributions over finite sets of outcomes that have no mean (= expected value)?\n",
    "\n",
    "**answer**\n",
    "\n",
    "*No*\n",
    "\n",
    "---\n",
    "\n",
    "You may want to refresh the notions of distributions defined by densities. We recommend *Probability* (1993) by Jim Pitman, section 4.1 (p.260ff) summarized on pp.262,263.\n",
    "\n",
    "**8.** Let $X$ be uniformly distributed over $[a,b]$. What is the expected value of $X$? Derive it using integrals.\n",
    "\n",
    "**answer**\n",
    "\n",
    "*Since X is uniformly distributed over $[a,b]$, then for $a < b$, $a < X < b$.* \n",
    "\n",
    "**9.** Let $X$ be uniformly distributed over $[0,1]$. What is the expected value of $X^3$? Derive it using integrals.\n",
    "\n",
    "**10.** Let $X$ be uniformly distributed over $[0,1]$. What is the expected value of $X^n$? Derive it using integrals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Transformations, Generalization\n",
    "\n",
    "These exercises will not be counted, but they are highly recommended for practice for the exam:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ex312.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ex314.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
