{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# 2022-05-11 Wednesday\n",
    "\n",
    "NAME: Ravi Moelchand, 746332\n",
    "\n",
    "Topics covered since the last tutorial:\n",
    "\n",
    "* Hoeffding's inequality (Lectures 3, 4)\n",
    "\n",
    "* Linear algebra (Lecture 3)\n",
    "\n",
    "* Adding probabilities, conditional probabilities, joint probabilities (Lectures 3, 4)\n",
    "\n",
    "* Linear classification, Pocket algorithm (Lecture 4)\n",
    "\n",
    "We will not cover \"adding probabilities, ...\" in this tutorial, because it will be covered in the next one.\n",
    "\n",
    "We also do not cover linear classification or the Pocket algorithm in this tutorial, and leave this for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "Before starting, recap the linear algebra definitions given at the end of lecture 3.\n",
    "\n",
    "Take two column vectors $\\vec{a}=\\langle 5,3,0\\rangle$, $\\vec{b}=\\langle 2,4,0\\rangle$\n",
    "\n",
    "**1.** Calculate $\\vec{a}\\cdot \\vec{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "$𝑎⃗ ⋅ 𝑏⃗ = ⟨5,3,0⟩ \\cdot ⟨2,4,0⟩ = 22$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Calculate the matrix product $\\vec{a}\\vec{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "not possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Calculate the matrix product $\\vec{a}^T\\vec{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "$  \\vec{a}^T\\vec{b} = ⟨5,3,0⟩^T \\cdot ⟨2,4,0⟩ = (22)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Calculate the matrix product $\\vec{b}\\vec{a}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "$ \\vec{b}\\vec{a}^T = ⟨2,4,0⟩ \\cdot ⟨5,3,0⟩^T = (22)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Is it true that $\\vec{a}\\vec{a}^T=\\vec{a}^T\\vec{a}$? Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "$\\vec{a}\\vec{a}^T = (34)$\n",
    "\n",
    "$\\vec{a}^T\\vec{a} = (34)$\n",
    "\n",
    "yes, because the order of multiplication does not matter in this case since they are the same vectors but one is transposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.** Is matrix division a well defined operation? For example, given matrices $A,B$, $M=\\frac{B}{A}$ iff $MA=B$. Make an explicit argument using example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "no, it is not a well defined operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoeffding Inequality\n",
    "\n",
    "\"Consider a bin that contains red and green marbles, possibly infinitely many. The proportion of red and green marbles in the bin is such that if we pick a marble at random, the probability that it will be red is $\\mu$ and the probability that it will be green is $1-\\mu$. We assume that the value of $\\mu$ is unknown to us.\n",
    "We pick a random sample of N independent marbles (with replacement) from this bin, and observe the fraction $\\nu$ of red marbles within the sample.\" (From: Learning From Data (Abu-Mostafa et al, 2012))\n",
    "\n",
    "**7.** If $\\mu = 0.7$, use the Hoeffding Inequality to bound the probability that a sample of 8 marbles will have $\\nu \\leq 0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "$\\nu \\leq 0,1 --- \\nu N \\leq 0,1N = 1 --- X \\leq 0,8$\n",
    "\n",
    "$X = 0$\n",
    "\n",
    "$\\begin{pmatrix} N\\\\ X\\\\ \\end{pmatrix} = \\begin{pmatrix} 8\\\\ 0\\\\ \\end{pmatrix} = \\frac{8!}{8!\\cdot 8!}$\n",
    "\n",
    "$P(X) = \\begin{pmatrix} N\\\\ X\\\\ \\end{pmatrix} \\cdot \\mu^{X} \\cdot (1 - \\mu)^{N-X}$\n",
    "\n",
    "$P(X) = \\frac{8!}{8!\\cdot 8!} \\cdot 0,7^{0} \\cdot (1 - 0,7)^{8-0}$\n",
    "\n",
    "$P(\\nu \\leq 0,1) = 2 \\cdot 10^{-9}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.** If $\\mu = 0.8$, find the smallest value for $N$ that one can find with the Hoeffding inequality for which holds that $\\nu$ deviates *at most* $0.05$ from $\\mu$ with a probability of at least $0.99$. (Note that the problem has been formulated in a reversed way from the book. In the book the probability represents the *undesired* situation that $\\mu$ and $\\nu$ deviate more than a predefined threshold. Here it is about the *desired* situation that they deviate at most a given threshold.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "$\\epsilon = 0,05\\\\\n",
    "0,99 = 2e^{-2\\epsilon^{2}N} = 2e^{-2 \\cdot 0,05^{2}N}\n",
    "$\n",
    "\n",
    "That gives $N = 141$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Hypotheses\n",
    "\n",
    "Suppose that you are confronted with a machine learning problem that is completely unknown to you. The data-set is offered by an alien from a completely different universe, and no context whatsoever is known. Assume that the target function is binary, and has an infinite input space, and could truly be any binary target function. (This is a stronger version of Exercise 1.12 from Learning From Data (Abu-Mostafa et al, 2012).)\n",
    "\n",
    "*Tip:* first thoroughly read Section 1.3.3 in the book. In particular, focus on \"The feasibility of learning is thus split into two questions\" up to the end of the section. That fragment explains an essential part of the intuition behind (statistical) machine learning.\n",
    "\n",
    "**9.** Answer Exercise 1.12 from the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "_b_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10.** Now, return to the alien's dataset. Suppose that you decide to put *any* possible function in your hypothesis set $\\mathcal{H}$ . Also, assume that you have some way to find the hypothesis $h$ with the lowest $E_{\\rm in}$.\n",
    "\n",
    "1. What can you say about the value of $E_{in}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "_Vanwege de oneindige inputruimte is er een grote hypotheseklasse en dat zorgt ervoor dat de kans dat er een hypothese is die goed met data overeenkomt groot is en dat $E_{in}$ klein is._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What can you say about how close $E_\\textrm{in}$ and $E_\\textrm{out}$ are together? So, what is the value of $P(|E_\\textrm{in} - E_\\textrm{out}| > \\epsilon)$. Consider any possible value for $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "_De kans dat $E_\\textrm{in} \\approx E_\\textrm{out}$ is klein, want de hypotheseklasse is groot. Ze liggen dus verder uit elkaar. Daarom zal $P(|E_\\textrm{in} - E_\\textrm{out}| > \\epsilon)$ groot zijn._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11.** Now, suppose that you decide to choose a less expressive hypothesis set. For example, you limit $\\mathcal{H}$ to all functions that a perceptron can learn.\n",
    "\n",
    "1. What can you say about the value of $E_\\textrm{in}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "_De hypotheseklasse wordt aanzienlijk kleiner dan in de vorige situatie, dus de waarde van $E_{in}$ zal relatief groot zijn._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What can you say about the \"generalisation of the error\" from in-of-sample to out-of-sample? So, what is the value of $P(|E_\\textrm{in} - E_\\textrm{out}| > \\epsilon)$. Consider any possible value for $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "_De kans dat $E_\\textrm{in} \\approx E_\\textrm{out}$ is nu groter, want de hypotheseklasse is kleiner. Ze liggen dus dichter bij elkaar. Daarom zal $P(|E_\\textrm{in} - E_\\textrm{out}| > \\epsilon)$ dichter bij de 0 zitten._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.** This solution is not provided in the multiple-choice question formed by Exercise 1.12. So, what does Exercise 1.12 tacitly assume?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "\n",
    "_That you approximate f well if you do find a g_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
